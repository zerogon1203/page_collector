# Page Collector

웹사이트의 여러 페이지를 순회하면서 본문 내용을 마크다운 형식으로 자동 수집하는 파이썬 도구입니다.

## 기능

- 여러 웹페이지를 순차적으로 방문하여 본문 내용 추출
- HTML을 마크다운 형식으로 자동 변환
- 수집된 내용을 개별 마크다운 파일로 저장
- JSON 설정 파일을 통한 자동화 지원
- 요청 간 지연 시간 설정으로 서버 부하 방지
- 진행 상황 표시 및 수집 결과 요약

## 설치

필요한 패키지를 설치합니다:

```bash
pip install -r requirements.txt
```

## 사용법

### 1. 대화형 모드 (수동 입력)

```bash
python page_collector.py
```

프로그램을 실행하면 다음 정보를 입력하라고 요청합니다:
- 기본 URL (예: https://example.com)
- 순회할 링크들 (한 줄에 하나씩, 빈 줄로 종료)
- 요청 간 지연 시간 (기본값: 1초)

### 2. 설정 파일 모드 (자동화)

`config.json` 파일을 생성하여 설정을 미리 정의할 수 있습니다:

```json
{
  "base_url": "https://example.com",
  "output_dir": "output",
  "delay": 1,
  "urls": [
    "/page1",
    "/page2", 
    "/blog/post1",
    "https://example.com/absolute-url"
  ]
}
```

그 후 프로그램 실행 시 설정 파일 사용을 선택합니다.

## 설정 파일 옵션

- `base_url`: 기본 URL (상대 경로 URL들이 이 주소를 기준으로 변환됩니다)
- `output_dir`: 결과 파일들이 저장될 디렉토리 (기본값: "output")
- `delay`: 요청 간 지연 시간(초) (기본값: 1)
- `urls`: 수집할 페이지 URL 목록 (상대 경로 또는 절대 경로)

## 출력 결과

프로그램 실행 후 다음 파일들이 생성됩니다:

### 개별 마크다운 파일
- `001_페이지제목.md`
- `002_다른페이지제목.md`
- ...

각 파일에는 다음 정보가 포함됩니다:
- 페이지 제목
- 원본 URL
- 수집 상태
- 수집 일시
- 본문 내용 (마크다운 형식)

### 수집 요약 파일
- `collection_summary.json`: 전체 수집 과정의 결과 요약

## 주요 특징

- **스마트 본문 추출**: 다양한 CSS 선택자를 사용하여 페이지의 주요 본문 내용을 자동으로 찾습니다
- **깔끔한 변환**: 불필요한 요소(스크립트, 스타일, 네비게이션 등)를 제거하고 순수한 본문만 추출
- **안전한 파일명**: 특수문자를 제거하고 길이를 제한하여 안전한 파일명 생성
- **에러 처리**: 접근할 수 없는 페이지나 오류 발생 시에도 계속 진행
- **진행 상황 표시**: tqdm을 사용한 진행률 표시

## 사용 예시

```bash
# 1. 패키지 설치
pip install -r requirements.txt

# 2. 프로그램 실행
python page_collector.py

# 3. 입력 예시
기본 URL을 입력하세요: https://docs.python.org
순회할 링크들을 입력하세요:
/3/tutorial/introduction.html
/3/tutorial/controlflow.html
/3/tutorial/datastructures.html
[빈 줄 입력으로 종료]
요청 간 지연 시간(초, 기본값 1): 2
```

## 주의사항

- 웹사이트의 이용약관과 robots.txt를 확인하여 수집이 허용되는지 확인하세요
- 서버에 과부하를 주지 않도록 적절한 지연 시간을 설정하세요
- 일부 동적 콘텐츠는 JavaScript가 필요할 수 있어 정확히 수집되지 않을 수 있습니다
